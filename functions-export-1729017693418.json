[{"id":"rate_limit_filter","user_id":"01503731-0a49-4033-bdc0-8c1bc3b75fd6","name":"Rate Limit Filter","type":"filter","content":"\"\"\"\ntitle: Rate Limit Filter\nauthor: justinh-rahb with improvements by Yanyutin753\nauthor_url: https://github.com/justinh-rahb\nfunding_url: https://github.com/open-webui\nversion: 0.2.1\nlicense: MIT\n\"\"\"\n\nimport time\nfrom typing import Optional, Tuple\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime, timedelta\n\n\nclass Filter:\n    class Valves(BaseModel):\n        priority: int = Field(\n            default=0, description=\"Priority level for the filter operations.\"\n        )\n        requests_per_minute: Optional[int] = Field(\n            default=10, description=\"Maximum number of requests allowed per minute.\"\n        )\n        requests_per_hour: Optional[int] = Field(\n            default=50, description=\"Maximum number of requests allowed per hour.\"\n        )\n        sliding_window_limit: Optional[int] = Field(\n            default=100,\n            description=\"Maximum number of requests allowed within the sliding window.\",\n        )\n        sliding_window_minutes: Optional[int] = Field(\n            default=180, description=\"Duration of the sliding window in minutes.\"\n        )\n        global_limit: bool = Field(\n            default=True,\n            description=\"Whether to apply the limits globally to all models.\",\n        )\n        enabled_for_admins: bool = Field(\n            default=True,\n            description=\"Whether rate limiting is enabled for admin users.\",\n        )\n\n    def __init__(self):\n        self.file_handler = False\n        self.valves = self.Valves()\n        self.user_requests = {}\n\n    def prune_requests(self, user_id: str, model_id: str):\n        now = time.time()\n\n        if user_id not in self.user_requests:\n            self.user_requests[user_id] = {}  # This remains a dict of model requests\n\n        if self.valves.global_limit:\n            # Clear all request timestamps for the user.\n            self.user_requests[user_id] = {\n                k: [\n                    req\n                    for req in v\n                    if (\n                        (self.valves.requests_per_minute is not None and now - req < 60)\n                        or (\n                            self.valves.requests_per_hour is not None\n                            and now - req < 3600\n                        )\n                        or (now - req < self.valves.sliding_window_minutes * 60)\n                    )\n                ]\n                for k, v in self.user_requests[user_id].items()\n            }\n        else:\n            # Clear request timestamps for the specified model only.\n            if model_id not in self.user_requests[user_id]:\n                return\n\n            self.user_requests[user_id][model_id] = [\n                req\n                for req in self.user_requests[user_id][model_id]\n                if (\n                    (self.valves.requests_per_minute is not None and now - req < 60)\n                    or (self.valves.requests_per_hour is not None and now - req < 3600)\n                    or (now - req < self.valves.sliding_window_minutes * 60)\n                )\n            ]\n\n    def rate_limited(\n        self, user_id: str, model_id: str\n    ) -> Tuple[bool, Optional[int], int]:\n        self.prune_requests(user_id, model_id)\n\n        if self.valves.global_limit:\n            user_reqs = self.user_requests.get(user_id, {})\n            requests_last_minute = sum(\n                1\n                for reqs in user_reqs.values()\n                for req in reqs\n                if time.time() - req < 60\n            )\n            if requests_last_minute >= self.valves.requests_per_minute:\n                earliest_request = min(\n                    req\n                    for reqs in user_reqs.values()\n                    for req in reqs\n                    if time.time() - req < 60\n                )\n                return (\n                    True,\n                    int(60 - (time.time() - earliest_request)),\n                    requests_last_minute,\n                )\n\n            requests_last_hour = sum(\n                1\n                for reqs in user_reqs.values()\n                for req in reqs\n                if time.time() - req < 3600\n            )\n            if requests_last_hour >= self.valves.requests_per_hour:\n                earliest_request = min(\n                    req\n                    for reqs in user_reqs.values()\n                    for req in reqs\n                    if time.time() - req < 3600\n                )\n                return (\n                    True,\n                    int(3600 - (time.time() - earliest_request)),\n                    requests_last_hour,\n                )\n\n            sliding_window_seconds = self.valves.sliding_window_minutes * 60\n            requests_in_window = sum(\n                1\n                for reqs in user_reqs.values()\n                for req in reqs\n                if time.time() - req < sliding_window_seconds\n            )\n            if requests_in_window >= self.valves.sliding_window_limit:\n                earliest_request = min(\n                    req\n                    for reqs in user_reqs.values()\n                    for req in reqs\n                    if time.time() - req < sliding_window_seconds\n                )\n                return (\n                    True,\n                    int(sliding_window_seconds - (time.time() - earliest_request)),\n                    requests_in_window,\n                )\n\n        # Process requests for a specific model.\n        if (\n            user_id not in self.user_requests\n            or model_id not in self.user_requests[user_id]\n        ):\n            return False, None, 0\n\n        user_reqs = self.user_requests[user_id][model_id]\n        requests_last_minute = sum(1 for req in user_reqs if time.time() - req < 60)\n        if requests_last_minute >= self.valves.requests_per_minute:\n            earliest_request = min(req for req in user_reqs if time.time() - req < 60)\n            return (\n                True,\n                int(60 - (time.time() - earliest_request)),\n                requests_last_minute,\n            )\n\n        requests_last_hour = sum(1 for req in user_reqs if time.time() - req < 3600)\n        if requests_last_hour >= self.valves.requests_per_hour:\n            earliest_request = min(req for req in user_reqs if time.time() - req < 3600)\n            return (\n                True,\n                int(3600 - (time.time() - earliest_request)),\n                requests_last_hour,\n            )\n\n        sliding_window_seconds = self.valves.sliding_window_minutes * 60\n        requests_in_window = sum(\n            1 for req in user_reqs if time.time() - req < sliding_window_seconds\n        )\n        if requests_in_window >= self.valves.sliding_window_limit:\n            earliest_request = min(\n                req for req in user_reqs if time.time() - req < sliding_window_seconds\n            )\n            return (\n                True,\n                int(sliding_window_seconds - (time.time() - earliest_request)),\n                requests_in_window,\n            )\n\n        return False, None, len(user_reqs)\n\n    def log_request(self, user_id: str, model_id: str):\n        if user_id not in self.user_requests:\n            self.user_requests[user_id] = {}\n        if model_id not in self.user_requests[user_id]:\n            self.user_requests[user_id][model_id] = []\n        self.user_requests[user_id][model_id].append(time.time())\n\n    def inlet(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __model__: Optional[dict] = None,\n    ) -> dict:\n        if __user__ is not None and (\n            __user__.get(\"role\") != \"admin\" or self.valves.enabled_for_admins\n        ):\n            user_id = __user__[\"id\"]\n            model_id = __model__[\"id\"] if __model__ is not None else \"default_model\"\n            rate_limited, wait_time, request_count = self.rate_limited(\n                user_id, model_id\n            )\n            if rate_limited:\n                current_time = datetime.now()\n                future_time = current_time + timedelta(seconds=wait_time)\n                future_time_str = future_time.strftime(\"%I:%M %p\")\n\n                raise Exception(\n                    f\"Rate limit exceeded. You have made {request_count} requests to model '{model_id}'. Please try again at {future_time_str}.\"\n                )\n\n            self.log_request(user_id, model_id)\n        return body\n","meta":{"description":"A filter that sets rate limits to prevent overuse of APIs.","manifest":{"title":"Rate Limit Filter","author":"justinh-rahb with improvements by Yanyutin753","author_url":"https://github.com/justinh-rahb","funding_url":"https://github.com/open-webui","version":"0.2.1","license":"MIT"}},"is_active":false,"is_global":true,"updated_at":1728999040,"created_at":1728009714},{"id":"google_translate","user_id":"01503731-0a49-4033-bdc0-8c1bc3b75fd6","name":"Google Translate","type":"filter","content":"\"\"\"\ntitle: Google Translate Filter\nauthor: justinh-rahb and OriginalSimon\nauthor_url: https://github.com/justinh-rahb\nfunding_url: https://github.com/open-webui\nversion: 0.1.5\nlicense: MIT\n\"\"\"\n\nimport re\nfrom typing import List, Optional\nfrom pydantic import BaseModel\nimport requests\nimport logging\n\nfrom open_webui.utils.misc import get_last_user_message, get_last_assistant_message\n\n# Configure logging\nlogging.basicConfig(level=logging.WARNING)  # Set the desired logging level\n\n\nclass Filter:\n    class Valves(BaseModel):\n        source_user: str = \"auto\"\n        target_user: str = \"en\"\n        source_assistant: str = \"en\"\n        target_assistant: str = \"en\"\n\n    def __init__(self) -> None:\n        self.valves = self.Valves()\n        self.code_blocks = []  # List to store code blocks\n\n    def translate(self, text: str, source: str, target: str) -> str:\n        url = \"https://translate.googleapis.com/translate_a/single\"\n        params = {\n            \"client\": \"gtx\",\n            \"sl\": source,\n            \"tl\": target,\n            \"dt\": \"t\",\n            \"q\": text,\n        }\n\n        try:\n            r = requests.get(\n                url, params=params, timeout=10\n            )  # Add timeout for robustness\n            r.raise_for_status()\n            result = r.json()\n            translated_text = \"\".join([sentence[0] for sentence in result[0]])\n            return translated_text\n        except requests.exceptions.RequestException as e:\n            error_msg = f\"Translation API error: {str(e)}\"\n            logging.error(error_msg)\n            return f\"{text}\\n\\n[Translation failed: {error_msg}]\"\n        except Exception as e:\n            error_msg = f\"Unexpected error during translation: {str(e)}\"\n            logging.exception(error_msg)  # Log traceback for unexpected errors\n            return f\"{text}\\n\\n[Translation failed: {error_msg}]\"\n\n    def split_text_around_table(self, text: str) -> List[str]:\n        table_regex = r\"((?:^.*?\\|.*?\\n)+)(?=\\n[^\\|\\s].*?\\|)\"\n        matches = re.split(table_regex, text, flags=re.MULTILINE)\n\n        if len(matches) > 1:\n            return [matches[0], matches[1]]\n        else:\n            return [text, \"\"]\n\n    def clean_table_delimiters(self, text: str) -> str:\n        # Replace multiple spaces around table delimiters with a single dash\n        return re.sub(r\"(\\|\\s*-+\\s*)+\", lambda m: m.group(0).replace(\" \", \"-\"), text)\n\n    async def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict:\n        print(f\"inlet:{__name__}\")\n        print(f\"source_user: {self.valves.source_user}\")\n        print(f\"target_user: {self.valves.target_user}\")\n        print(f\"source_assistant: {self.valves.source_assistant}\")\n        print(f\"target_assistant: {self.valves.target_assistant}\")\n\n        user_message = get_last_user_message(body[\"messages\"])\n\n        # Find and store code blocks\n        code_regex = r\"```(.*?)```\"\n        self.code_blocks = re.findall(code_regex, user_message, flags=re.DOTALL)\n\n        # Replace code blocks with placeholders temporarily\n        user_message_processed = re.sub(\n            code_regex, \"__CODE_BLOCK__\", user_message, flags=re.DOTALL\n        )\n\n        if self.valves.source_user != self.valves.target_user:\n            parts = self.split_text_around_table(user_message_processed)\n            text_before_table, table_text = parts\n\n            translated_before_table = self.translate(\n                text_before_table,\n                self.valves.source_user,\n                self.valves.target_user,\n            )\n\n            translated_user_message = translated_before_table + table_text\n            translated_user_message = self.clean_table_delimiters(\n                translated_user_message\n            )\n\n            # Restore code blocks in translated message\n            for code in self.code_blocks:\n                translated_user_message = translated_user_message.replace(\n                    \"__CODE_BLOCK__\", f\"```{code}```\", 1\n                )\n\n            for message in reversed(body[\"messages\"]):\n                if message[\"role\"] == \"user\":\n                    if \"[Translation failed:\" in translated_user_message:\n                        print(\n                            f\"Translation failed for language pair {self.valves.source_user} to {self.valves.target_user}\"\n                        )\n                        # Optionally, you could decide not to update the message content if translation failed\n                        # return body\n                    else:\n                        message[\"content\"] = translated_user_message\n                    break\n\n        return body\n\n    async def outlet(self, body: dict, __user__: Optional[dict] = None) -> dict:\n        print(f\"outlet:{__name__}\")\n        print(f\"source_user: {self.valves.source_user}\")\n        print(f\"target_user: {self.valves.target_user}\")\n        print(f\"source_assistant: {self.valves.source_assistant}\")\n        print(f\"target_assistant: {self.valves.target_assistant}\")\n\n        assistant_message = get_last_assistant_message(body[\"messages\"])\n\n        # Find and store code blocks\n        code_regex = r\"```(.*?)```\"\n        self.code_blocks = re.findall(code_regex, assistant_message, flags=re.DOTALL)\n\n        # Replace code blocks with placeholders temporarily\n        assistant_message_processed = re.sub(\n            code_regex, \"__CODE_BLOCK__\", assistant_message, flags=re.DOTALL\n        )\n\n        if self.valves.source_assistant != self.valves.target_assistant:\n            parts = self.split_text_around_table(assistant_message_processed)\n            text_before_table, table_text = parts\n\n            translated_before_table = self.translate(\n                text_before_table,\n                self.valves.source_assistant,\n                self.valves.target_assistant,\n            )\n\n            translated_assistant_message = translated_before_table + table_text\n            translated_assistant_message = self.clean_table_delimiters(\n                translated_assistant_message\n            )\n\n            # Restore code blocks in translated message\n            for code in self.code_blocks:\n                translated_assistant_message = translated_assistant_message.replace(\n                    \"__CODE_BLOCK__\", f\"```{code}```\", 1\n                )\n\n            for message in reversed(body[\"messages\"]):\n                if message[\"role\"] == \"assistant\":\n                    if \"[Translation failed:\" in translated_assistant_message:\n                        print(\n                            f\"Translation failed for language pair {self.valves.source_assistant} to {self.valves.target_assistant}\"\n                        )\n                        # Optionally, you could decide not to update the message content if translation failed\n                        # return body\n                    else:\n                        message[\"content\"] = translated_assistant_message\n                    break\n\n        return body\n","meta":{"description":"Uses Google translation API to translate from a user's native language to the LLM's native language, and back again to the user's.","manifest":{"title":"Google Translate Filter","author":"justinh-rahb and OriginalSimon","author_url":"https://github.com/justinh-rahb","funding_url":"https://github.com/open-webui","version":"0.1.5","license":"MIT"}},"is_active":true,"is_global":false,"updated_at":1728840785,"created_at":1728010104},{"id":"dall_e","user_id":"01503731-0a49-4033-bdc0-8c1bc3b75fd6","name":"DALL-E","type":"pipe","content":"\"\"\"\ntitle: DALLÂ·E Manifold\ndescription: A manifold function to integrate OpenAI's DALL-E models into Open WebUI.\nauthor: bgeneto (based on Marc Lopez pipeline)\nfunding_url: https://github.com/open-webui\nversion: 0.1.4\nlicense: MIT\nrequirements: pydantic\nenvironment_variables: OPENAI_API_BASE_URL, OPENAI_IMG_API_KEY\n\"\"\"\n\nimport os\nfrom typing import Iterator, List, Union\n\nfrom open_webui.utils.misc import get_last_user_message\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\n\nclass Pipe:\n    \"\"\"OpenAI ImageGen pipeline\"\"\"\n\n    class Valves(BaseModel):\n\n        OPENAI_API_BASE_URL: str = Field(\n            default=\"https://api.openai.com/v1\", description=\"OpenAI API Base URL\"\n        )\n        OPENAI_IMG_API_KEY: str = Field(default=\"\", description=\"your OpenAI API key\")\n        IMAGE_SIZE: str = Field(default=\"1024x1024\", description=\"Generated image size\")\n        NUM_IMAGES: int = Field(default=1, description=\"Number of images to generate\")\n\n    def __init__(self):\n        self.type = \"manifold\"\n        self.id = \"DALL_E\"\n        self.name = \"DALL-E\"\n        self.valves = self.Valves(\n            **{\n                \"OPENAI_IMG_API_KEY\": os.getenv(\"OPENAI_IMG_API_KEY\", \"\"),\n                \"OPENAI_API_BASE_URL\": os.getenv(\n                    \"OPENAI_API_BASE_URL\", \"https://api.openai.com/v1\"\n                ),\n            }\n        )\n\n        self.client = OpenAI(\n            base_url=self.valves.OPENAI_API_BASE_URL,\n            api_key=self.valves.OPENAI_IMG_API_KEY,\n        )\n\n    def get_openai_assistants(self) -> List[dict]:\n        \"\"\"Get the available ImageGen models from OpenAI\n\n        Returns:\n            List[dict]: The list of ImageGen models\n        \"\"\"\n\n        if self.valves.OPENAI_IMG_API_KEY:\n            self.client = OpenAI(\n                base_url=self.valves.OPENAI_API_BASE_URL,\n                api_key=self.valves.OPENAI_IMG_API_KEY,\n            )\n\n            models = self.client.models.list()\n            return [\n                {\n                    \"id\": model.id,\n                    \"name\": model.id,\n                }\n                for model in models\n                if \"dall-e\" in model.id\n            ]\n\n        return []\n\n    def pipes(self) -> List[dict]:\n        return self.get_openai_assistants()\n\n    def pipe(self, body: dict) -> Union[str, Iterator[str]]:\n        if not self.valves.OPENAI_IMG_API_KEY:\n            return \"Error: OPENAI_IMG_API_KEY is not set\"\n\n        self.client = OpenAI(\n            base_url=self.valves.OPENAI_API_BASE_URL,\n            api_key=self.valves.OPENAI_IMG_API_KEY,\n        )\n\n        model_id = body[\"model\"]\n        model_id = model_id.split(\".\")[1]\n        user_message = get_last_user_message(body[\"messages\"])\n\n        response = self.client.images.generate(\n            model=model_id,\n            prompt=user_message,\n            size=self.valves.IMAGE_SIZE,\n            n=self.valves.NUM_IMAGES,\n        )\n\n        message = \"\"\n        for image in response.data:\n            if image.url:\n                message += \"![image](\" + image.url + \")\\n\"\n\n        yield message\n","meta":{"description":"A manifold function to integrate OpenAI's DALL-E models into Open WebUI","manifest":{"title":"DALLÂ·E Manifold","description":"A manifold function to integrate OpenAI's DALL-E models into Open WebUI.","author":"bgeneto (based on Marc Lopez pipeline)","funding_url":"https://github.com/open-webui","version":"0.1.4","license":"MIT","requirements":"pydantic","environment_variables":"OPENAI_API_BASE_URL, OPENAI_IMG_API_KEY"}},"is_active":true,"is_global":false,"updated_at":1729017598,"created_at":1728013845},{"id":"cost_tracker","user_id":"01503731-0a49-4033-bdc0-8c1bc3b75fd6","name":"Cost Tracker","type":"filter","content":"\"\"\"\ntitle: Cost Tracker\ndescription: This function is designed to manage and calculate the costs associated with user interactions and model usage in a Open WebUI.\nauthor: bgeneto\nauthor_url: https://github.com/bgeneto/open-webui-cost-tracker\nfunding_url: https://github.com/open-webui\nversion: 0.2.2\nlicense: MIT\nrequirements: requests, tiktoken, cachetools, pydantic\nenvironment_variables:\ndisclaimer: This function is provided as is without any guarantees.\n            It is your responsibility to ensure that the function meets your requirements.\n            All metrics and costs are approximate and may vary depending on the model and the usage.\n\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport time\nfrom datetime import datetime\nfrom decimal import ROUND_HALF_UP, Decimal\nfrom threading import Lock\nfrom typing import Any, Awaitable, Callable, Optional\n\nimport requests\nimport tiktoken\nfrom cachetools import TTLCache, cached\nfrom open_webui.utils.misc import get_last_assistant_message, get_messages_content\nfrom pydantic import BaseModel, Field\n\n\nclass Config:\n    DATA_DIR = \"data\"\n    CACHE_DIR = os.path.join(DATA_DIR, \".cache\")\n    USER_COST_FILE = os.path.join(DATA_DIR, f\"costs-{datetime.now().year}.json\")\n    CACHE_TTL = 432000  # try to keep model pricing json file for 5 days in the cache.\n    CACHE_MAXSIZE = 16\n    DECIMALS = \"0.00000001\"\n    DEBUG_PREFIX = \"DEBUG:    \" + __name__.upper() + \" -\"\n    INFO_PREFIX = \"INFO:     \" + __name__.upper() + \" -\"\n    DEBUG = False\n\n\n# Initialize cache\ncache = TTLCache(maxsize=Config.CACHE_MAXSIZE, ttl=Config.CACHE_TTL)\n\n\ndef get_encoding(model):\n    try:\n        return tiktoken.encoding_for_model(model)\n    except KeyError:\n        if Config.DEBUG:\n            print(\n                f\"{Config.DEBUG_PREFIX} Encoding for model {model} not found. Using cl100k_base for computing tokens.\"\n            )\n        return tiktoken.get_encoding(\"cl100k_base\")\n\n\nclass UserCostManager:\n    def __init__(self, cost_file_path):\n        self.cost_file_path = cost_file_path\n        self._ensure_cost_file_exists()\n\n    def _ensure_cost_file_exists(self):\n        if not os.path.exists(self.cost_file_path):\n            with open(self.cost_file_path, \"w\", encoding=\"UTF-8\") as cost_file:\n                json.dump({}, cost_file)\n\n    def _read_costs(self):\n        with open(self.cost_file_path, \"r\", encoding=\"UTF-8\") as cost_file:\n            return json.load(cost_file)\n\n    def _write_costs(self, costs):\n        with open(self.cost_file_path, \"w\", encoding=\"UTF-8\") as cost_file:\n            json.dump(costs, cost_file, indent=4)\n\n    def update_user_cost(\n        self,\n        user_email: str,\n        model: str,\n        input_tokens: int,\n        output_tokens: int,\n        total_cost: Decimal,\n    ):\n        costs = self._read_costs()\n        timestamp = datetime.now().isoformat()\n\n        if user_email not in costs:\n            costs[user_email] = []\n\n        costs[user_email].append(\n            {\n                \"model\": model,\n                \"timestamp\": timestamp,\n                \"input_tokens\": input_tokens,\n                \"output_tokens\": output_tokens,\n                \"total_cost\": str(total_cost),\n            }\n        )\n\n        self._write_costs(costs)\n\n\nclass ModelCostManager:\n    _best_match_cache = {}\n\n    def __init__(self, cache_dir=Config.CACHE_DIR):\n        self.cache_dir = cache_dir\n        self.lock = Lock()\n        self.url = \"https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json\"\n        self.cache_file_path = self._get_cache_filename()\n        self._ensure_cache_dir()\n\n    def _ensure_cache_dir(self):\n        if not os.path.exists(self.cache_dir):\n            os.makedirs(self.cache_dir)\n\n    def _get_cache_filename(self):\n        cache_file_name = hashlib.sha256(self.url.encode()).hexdigest() + \".json\"\n        return os.path.normpath(os.path.join(self.cache_dir, cache_file_name))\n\n    def _is_cache_valid(self, cache_file_path):\n        cache_file_mtime = os.path.getmtime(cache_file_path)\n        return time.time() - cache_file_mtime < cache.ttl\n\n    @cached(cache=cache)\n    def get_cost_data(self):\n        \"\"\"\n        Fetches a JSON file from a URL and stores it in cache.\n\n        This method attempts to retrieve a JSON file from the specified URL. To optimize performance and reduce\n        network requests, it caches the JSON data locally. If the cached data is available and still valid,\n        it returns the cached data instead of making a new network request. If the cached data is not available\n        or has expired, it fetches the data from the URL, caches it, and then returns it.\n\n        Returns:\n            dict: The JSON data retrieved from the URL or the cache.\n\n        Raises:\n            requests.RequestException: If the network request fails and no valid cache is available.\n        \"\"\"\n\n        with self.lock:\n            if os.path.exists(self.cache_file_path) and self._is_cache_valid(\n                self.cache_file_path\n            ):\n                with open(self.cache_file_path, \"r\", encoding=\"UTF-8\") as cache_file:\n                    if Config.DEBUG:\n                        print(\n                            f\"{Config.DEBUG_PREFIX} Reading costs json file from disk!\"\n                        )\n                    return json.load(cache_file)\n        try:\n            if Config.DEBUG:\n                print(f\"{Config.DEBUG_PREFIX} Downloading model costs json file!\")\n            response = requests.get(self.url)\n            response.raise_for_status()\n            data = response.json()\n\n            # backup existing cache file\n            try:\n                if os.path.exists(self.cache_file_path):\n                    os.rename(self.cache_file_path, self.cache_file_path + \".bkp\")\n            except Exception as e:\n                print(f\"**ERROR: Failed to backup costs json file. Error: {e}\")\n\n            with self.lock:\n                with open(self.cache_file_path, \"w\", encoding=\"UTF-8\") as cache_file:\n                    if Config.DEBUG:\n                        print(f\"{Config.DEBUG_PREFIX} Writing costs to json file!\")\n                    json.dump(data, cache_file)\n\n            return data\n        except Exception as e:\n            print(\n                f\"**ERROR: Failed to download or write to costs json file. Using old cached file if available. Error: {e}\"\n            )\n            with self.lock:\n                if os.path.exists(self.cache_file_path + \".bkp\"):\n                    with open(\n                        self.cache_file_path + \".bkp\", \"r\", encoding=\"UTF-8\"\n                    ) as cache_file:\n                        if Config.DEBUG:\n                            print(\n                                f\"{Config.DEBUG_PREFIX} Reading costs json file from backup!\"\n                            )\n                        return json.load(cache_file)\n                else:\n                    raise e\n\n    def levenshtein_distance(self, s1, s2):\n        m, n = len(s1), len(s2)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n        for i in range(m + 1):\n            dp[i][0] = i\n        for j in range(n + 1):\n            dp[0][j] = j\n\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                cost = 0 if s1[i - 1] == s2[j - 1] else 1\n                dp[i][j] = min(\n                    dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost\n                )\n\n        return dp[m][n]\n\n    def _find_best_match(self, query: str, json_data) -> str:\n        # Exact match search\n        query_lower = query.lower()\n        keys_lower = {key.lower(): key for key in json_data.keys()}\n\n        if query_lower in keys_lower:\n            return keys_lower[query_lower]\n\n        # Fallback to Levenshtein distance matching if no exact match is found\n        threshold_ratio = 0.6 if len(query) < 15 else 0.3\n        min_distance = float(\"inf\")\n        best_match = None\n        threshold = round(len(query) * threshold_ratio)\n\n        start = time.time()\n        distances = (self.levenshtein_distance(query_lower, key) for key in keys_lower)\n        for key, dist in zip(keys_lower.values(), distances):\n            if dist < min_distance:\n                min_distance = dist\n                best_match = key\n            if dist < 2:  # Early termination for (almost) exact match\n                return key\n        end = time.time()\n        if Config.DEBUG:\n            print(\n                f\"{Config.DEBUG_PREFIX} Levenshtein distance search took {end - start:.3f} seconds\"\n            )\n        if min_distance > threshold:\n            return None  # No match found within the threshold\n\n        return best_match\n\n    def get_model_data(self, model):\n        json_data = self.get_cost_data()\n\n        if model in ModelCostManager._best_match_cache:\n            if Config.DEBUG:\n                print(\n                    f\"{Config.DEBUG_PREFIX} Using cached costs for model named '{model}'\"\n                )\n            best_match = ModelCostManager._best_match_cache[model]\n        else:\n            if Config.DEBUG:\n                print(\n                    f\"{Config.DEBUG_PREFIX} Searching best match in costs file for model named '{model}'\"\n                )\n            best_match = self._find_best_match(model, json_data)\n            ModelCostManager._best_match_cache[model] = best_match\n\n        if best_match is None:\n            return {}\n\n        if Config.DEBUG:\n            print(f\"{Config.DEBUG_PREFIX} Using costs from '{best_match}'\")\n\n        return json_data.get(best_match, {})\n\n\nclass CostCalculator:\n    def __init__(\n        self, user_cost_manager: UserCostManager, model_cost_manager: ModelCostManager\n    ):\n        self.model_cost_manager = model_cost_manager\n        self.user_cost_manager = user_cost_manager\n\n    def calculate_costs(\n        self, model: str, input_tokens: int, output_tokens: int, compensation: float\n    ) -> Decimal:\n        model_pricing_data = self.model_cost_manager.get_model_data(model)\n        if not model_pricing_data:\n            print(f\"{Config.INFO_PREFIX} Model '{model}' not found in costs json file!\")\n        input_cost_per_token = Decimal(\n            str(model_pricing_data.get(\"input_cost_per_token\", 0))\n        )\n        output_cost_per_token = Decimal(\n            str(model_pricing_data.get(\"output_cost_per_token\", 0))\n        )\n\n        input_cost = input_tokens * input_cost_per_token\n        output_cost = output_tokens * output_cost_per_token\n        total_cost = Decimal(float(compensation)) * (input_cost + output_cost)\n        total_cost = total_cost.quantize(\n            Decimal(Config.DECIMALS), rounding=ROUND_HALF_UP\n        )\n\n        return total_cost\n\n\nclass Filter:\n    class Valves(BaseModel):\n        priority: int = Field(default=15, description=\"Priority level\")\n        compensation: float = Field(\n            default=1.0, description=\"Compensation for price calculation (percent)\"\n        )\n        elapsed_time: bool = Field(default=True, description=\"Display the elapsed time\")\n        number_of_tokens: bool = Field(\n            default=True, description=\"Display total number of tokens\"\n        )\n        tokens_per_sec: bool = Field(\n            default=True, description=\"Display tokens per second metric\"\n        )\n        debug: bool = Field(default=False, description=\"Display debugging messages\")\n        pass\n\n    def __init__(self):\n        self.valves = self.Valves()\n        Config.DEBUG = self.valves.debug\n        self.model_cost_manager = ModelCostManager()\n        self.user_cost_manager = UserCostManager(Config.USER_COST_FILE)\n        self.cost_calculator = CostCalculator(\n            self.user_cost_manager, self.model_cost_manager\n        )\n        self.start_time = None\n        self.input_tokens = 0\n        pass\n\n    def _sanitize_model_name(self, name: str) -> str:\n        \"\"\"Sanitize model name by removing prefixes and suffixes\n\n        Args:\n            name (str): model name\n\n        Returns:\n            str: sanitized model name\n        \"\"\"\n        prefixes = [\n            \"openai\",\n            \"github\",\n            \"google_genai\",\n        ]\n        suffixes = [\"-tuned\"]\n        # remove prefixes and suffixes\n        for prefix in prefixes:\n            if name.startswith(prefix):\n                name = name[len(prefix) :]\n        for suffix in suffixes:\n            if name.endswith(suffix):\n                name = name[: -len(suffix)]\n        return name.lower().strip()\n\n    def _remove_roles(self, content):\n        # Define the roles to be removed\n        roles = [\"SYSTEM:\", \"USER:\", \"ASSISTANT:\", \"PROMPT:\"]\n\n        # Process each line\n        def process_line(line):\n            for role in roles:\n                if line.startswith(role):\n                    return line.split(\":\", 1)[1].strip()\n            return line  # Return the line unchanged if no role matches\n\n        return \"\\n\".join([process_line(line) for line in content.split(\"\\n\")])\n\n    def _get_model(self, body):\n        if \"model\" in body:\n            return self._sanitize_model_name(body[\"model\"])\n        return None\n\n    async def inlet(\n        self,\n        body: dict,\n        __event_emitter__: Callable[[Any], Awaitable[None]] = None,\n        __model__: Optional[dict] = None,\n        __user__: Optional[dict] = None,\n    ) -> dict:\n\n        Config.DEBUG = self.valves.debug\n        enc = tiktoken.get_encoding(\"cl100k_base\")\n        input_content = self._remove_roles(\n            get_messages_content(body[\"messages\"])\n        ).strip()\n        self.input_tokens = len(enc.encode(input_content))\n\n        await __event_emitter__(\n            {\n                \"type\": \"status\",\n                \"data\": {\n                    \"description\": f\"Processing {self.input_tokens} input tokens...\",\n                    \"done\": False,\n                },\n            }\n        )\n\n        # add user email to payload in order to track costs\n        if __user__:\n            if \"email\" in __user__:\n                if Config.DEBUG:\n                    print(\n                        f\"{Config.DEBUG_PREFIX} Adding email to request body: {__user__['email']}\"\n                    )\n                body[\"user\"] = __user__[\"email\"]\n\n        self.start_time = time.time()\n\n        return body\n\n    async def outlet(\n        self,\n        body: dict,\n        __event_emitter__: Callable[[Any], Awaitable[None]],\n        __model__: Optional[dict] = None,\n        __user__: Optional[dict] = None,\n    ) -> dict:\n\n        end_time = time.time()\n        elapsed_time = end_time - self.start_time\n\n        await __event_emitter__(\n            {\n                \"type\": \"status\",\n                \"data\": {\n                    \"description\": \"Computing number of output tokens...\",\n                    \"done\": False,\n                },\n            }\n        )\n\n        model = self._get_model(body)\n        enc = tiktoken.get_encoding(\"cl100k_base\")\n        output_tokens = len(enc.encode(get_last_assistant_message(body[\"messages\"])))\n\n        await __event_emitter__(\n            {\n                \"type\": \"status\",\n                \"data\": {\"description\": \"Computing total costs...\", \"done\": False},\n            }\n        )\n\n        total_cost = self.cost_calculator.calculate_costs(\n            model, self.input_tokens, output_tokens, self.valves.compensation\n        )\n\n        if __user__:\n            if \"email\" in __user__:\n                user_email = __user__[\"email\"]\n            else:\n                print(\"**ERROR: User email not found!\")\n            try:\n                self.user_cost_manager.update_user_cost(\n                    user_email,\n                    model,\n                    self.input_tokens,\n                    output_tokens,\n                    total_cost,\n                )\n            except Exception as _:\n                print(\"**ERROR: Unable to update user cost file!\")\n        else:\n            print(\"**ERROR: User not found!\")\n\n        tokens = self.input_tokens + output_tokens\n        tokens_per_sec = tokens / elapsed_time\n        stats_array = []\n\n        if self.valves.elapsed_time:\n            stats_array.append(f\"{elapsed_time:.2f} s\")\n        if self.valves.tokens_per_sec:\n            stats_array.append(f\"{tokens_per_sec:.2f} T/s\")\n        if self.valves.number_of_tokens:\n            stats_array.append(f\"{tokens} Tokens\")\n\n        if float(total_cost) < float(Config.DECIMALS):\n            stats_array.append(f\"${total_cost:.2f}\")\n        else:\n            stats_array.append(f\"${total_cost:.6f}\")\n\n        stats = \" | \".join(stats_array)\n\n        await __event_emitter__(\n            {\"type\": \"status\", \"data\": {\"description\": stats, \"done\": True}}\n        )\n\n        return body\n","meta":{"description":"This function calculates API interaction pricing in Open WebUI and records user model usage.","manifest":{"title":"Cost Tracker","description":"This function is designed to manage and calculate the costs associated with user interactions and model usage in a Open WebUI.","author":"bgeneto","author_url":"https://github.com/bgeneto/open-webui-cost-tracker","funding_url":"https://github.com/open-webui","version":"0.2.2","license":"MIT","requirements":"requests, tiktoken, cachetools, pydantic","environment_variables":"","disclaimer":"This function is provided as is without any guarantees."}},"is_active":false,"is_global":true,"updated_at":1728999031,"created_at":1728114547},{"id":"flux_schnell","user_id":"01503731-0a49-4033-bdc0-8c1bc3b75fd6","name":"FLUX Schnell","type":"pipe","content":"\"\"\"\ntitle: FLUX.1 Schnell Manifold Function for Black Forest Lab Image Generation Model\nauthor: bgeneto\nauthor_url: https://github.com/bgeneto/open-webui-flux-image-gen\nfunding_url: https://github.com/open-webui\nversion: 0.2.1\nlicense: MIT\nrequirements: pydantic, requests\nenvironment_variables: TOGETHER_API_KEY, HUGGINGFACE_API_KEY, REPLICATE_API_KEY\nsupported providers: huggingface.co, replicate.com, together.xyz\nproviders urls:\nhttps://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell\nhttps://api.replicate.com/v1/models/black-forest-labs/flux-schnell/predictions\nhttps://api.together.xyz/v1/images/generations\n\"\"\"\n\nimport base64\nimport os\nfrom typing import Any, Dict, Generator, Iterator, List, Union\n\nimport requests\nfrom open_webui.utils.misc import get_last_user_message\nfrom pydantic import BaseModel, Field\n\n\nclass Provider:\n    def __init__(\n        self, name: str, url: str, headers: Dict[str, str], payload: Dict[str, Any]\n    ):\n        self.name = name\n        self.url = url\n        self.headers = headers\n        self.payload = payload\n\n\nclass Pipe:\n    \"\"\"\n    Class representing the FLUX.1 Schnell Manifold Function.\n    \"\"\"\n\n    class Valves(BaseModel):\n        \"\"\"\n        Pydantic model for storing API keys and base URLs.\n        \"\"\"\n\n        TOGETHER_API_KEY: str = Field(\n            default=\"\", description=\"Your API Key for Together.xyz\"\n        )\n        HUGGINGFACE_API_KEY: str = Field(\n            default=\"\", description=\"Your API Key for Huggingface.co\"\n        )\n        REPLICATE_API_KEY: str = Field(\n            default=\"\", description=\"Your API Key for Replicate.com\"\n        )\n\n    def __init__(self):\n        \"\"\"\n        Initialize the Pipe class with default values and environment variables.\n        \"\"\"\n        self.type = \"manifold\"\n        self.id = \"FLUX_Schnell\"\n        self.name = \"FLUX.1: \"\n        self.valves = self.Valves(\n            TOGETHER_API_KEY=os.getenv(\"TOGETHER_API_KEY\", \"\"),\n            HUGGINGFACE_API_KEY=os.getenv(\"HUGGINGFACE_API_KEY\", \"\"),\n            REPLICATE_API_KEY=os.getenv(\"REPLICATE_API_KEY\", \"\"),\n        )\n\n        self.providers = [\n            Provider(\n                name=\"together.xyz\",\n                url=\"https://api.together.xyz/v1/images/generations\",\n                headers={\n                    \"Authorization\": f\"Bearer {self.valves.TOGETHER_API_KEY}\",\n                    \"Content-Type\": \"application/json\",\n                },\n                payload={\n                    \"model\": \"black-forest-labs/FLUX.1-schnell-Free\",\n                    \"width\": 1024,\n                    \"height\": 1024,\n                    \"steps\": 4,\n                    \"n\": 1,\n                    \"response_format\": \"b64_json\",\n                },\n            ),\n            Provider(\n                name=\"huggingface.co\",\n                url=\"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell\",\n                headers={\n                    \"Authorization\": f\"Bearer {self.valves.HUGGINGFACE_API_KEY}\",\n                    \"Content-Type\": \"application/json\",\n                    \"x-wait-for-model\": \"true\",\n                },\n                payload={},\n            ),\n            Provider(\n                name=\"replicate.com\",\n                url=\"https://api.replicate.com/v1/models/black-forest-labs/flux-schnell/predictions\",\n                headers={\n                    \"Authorization\": f\"Bearer {self.valves.REPLICATE_API_KEY}\",\n                    \"Content-Type\": \"application/json\",\n                    \"Prefer\": \"wait=15\",\n                },\n                payload={\n                    \"input\": {\n                        \"go_fast\": True,\n                        \"num_outputs\": 1,\n                        \"aspect_ratio\": \"1:1\",\n                        \"output_format\": \"webp\",\n                        \"output_quality\": 90,\n                    }\n                },\n            ),\n        ]\n\n    def url_to_img_data(self, API_KEY: str, url: str) -> str:\n        \"\"\"\n        Convert a URL to base64-encoded image data.\n\n        Args:\n            url (str): The URL of the image.\n\n        Returns:\n            str: Base64-encoded image data.\n        \"\"\"\n        headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n\n        content_type = response.headers.get(\"Content-Type\", \"application/octet-stream\")\n        encoded_content = base64.b64encode(response.content).decode(\"utf-8\")\n        return f\"data:{content_type};base64,{encoded_content}\"\n\n    def non_stream_response(self, provider: Provider) -> str:\n        \"\"\"\n        Get a non-streaming response from the API.\n\n        Args:\n            provider (Provider): The provider details.\n\n        Returns:\n            str: The response from the API.\n        \"\"\"\n        try:\n            response = requests.post(\n                url=provider.url,\n                headers=provider.headers,\n                json=provider.payload,\n                stream=False,\n                timeout=(4.05, 20),\n            )\n            response.raise_for_status()\n\n            content_type = response.headers.get(\"Content-Type\", \"\")\n            if \"application/json\" in content_type:\n                return self.handle_json_response(response)\n            elif \"image/\" in content_type:\n                return self.handle_image_response(response)\n            else:\n                return f\"Error: Unsupported content type {content_type}\"\n\n        except requests.exceptions.RequestException as e:\n            return f\"Error: Request failed: {e}\"\n        except Exception as e:\n            return f\"Error: {e}\"\n\n    def stream_response(self, provider: Provider) -> Generator[str, None, None]:\n        yield self.non_stream_response(provider)\n\n    def get_img_extension(self, img_data: str) -> Union[str, None]:\n        \"\"\"\n        Get the image extension based on the base64-encoded data.\n\n        Args:\n            img_data (str): Base64-encoded image data.\n\n        Returns:\n            Union[str, None]: The image extension or None if unsupported.\n        \"\"\"\n        if img_data.startswith(\"/9j/\"):\n            return \"jpeg\"\n        elif img_data.startswith(\"iVBOR\"):\n            return \"png\"\n        elif img_data.startswith(\"R0lG\"):\n            return \"gif\"\n        elif img_data.startswith(\"UklGR\"):\n            return \"webp\"\n        return None\n\n    def handle_json_response(self, response: requests.Response) -> str:\n        \"\"\"\n        Handle JSON response from the API.\n\n        Args:\n            response (requests.Response): The response object.\n\n        Returns:\n            str: The formatted image data or an error message.\n        \"\"\"\n        resp = response.json()\n        if \"output\" in resp:\n            img_data = resp[\"output\"][0]\n        elif \"data\" in resp and \"b64_json\" in resp[\"data\"][0]:\n            img_data = resp[\"data\"][0][\"b64_json\"]\n        else:\n            return \"Error: Unexpected response format for the image provider!\"\n\n        # split ;base64, from img_data\n        try:\n            img_data = img_data.split(\";base64,\")[1]\n        except IndexError:\n            pass\n\n        img_ext = self.get_img_extension(img_data[:9])\n        if not img_ext:\n            return \"Error: Unsupported image format!\"\n\n        # rebuild img_data with proper format\n        img_data = f\"data:image/{img_ext};base64,{img_data}\"\n        return f\"![Image]({img_data})\\n`GeneratedImage.{img_ext}`\"\n\n    def handle_image_response(self, response: requests.Response) -> str:\n        \"\"\"\n        Handle image response from the API.\n\n        Args:\n            response (requests.Response): The response object.\n\n        Returns:\n            str: The formatted image data.\n        \"\"\"\n        content_type = response.headers.get(\"Content-Type\", \"\")\n        # check image type in the content type\n        img_ext = \"png\"\n        if \"image/\" in content_type:\n            img_ext = content_type.split(\"/\")[-1]\n        image_base64 = base64.b64encode(response.content).decode(\"utf-8\")\n        return f\"![Image](data:{content_type};base64,{image_base64})\\n`GeneratedImage.{img_ext}`\"\n\n    def pipes(self) -> List[Dict[str, str]]:\n        \"\"\"\n        Get the list of available pipes.\n\n        Returns:\n            List[Dict[str, str]]: The list of pipes.\n        \"\"\"\n        return [{\"id\": \"flux_schnell\", \"name\": \"Schnell\"}]\n\n    def pipe(\n        self, body: Dict[str, Any]\n    ) -> Union[str, Generator[str, None, None], Iterator[str]]:\n\n        prompt = get_last_user_message(body[\"messages\"])\n\n        for provider in self.providers:\n            provider.payload[\"prompt\"] = prompt\n            try:\n                if body.get(\"stream\", False):\n                    response = self.stream_response(provider)\n                else:\n                    response = self.non_stream_response(provider)\n                print(\"Image Provider:\", provider.name)\n                return response\n            except requests.exceptions.RequestException:\n                continue\n            except Exception as e:\n                return f\"Error: {e}\"\n\n        return \"Error: All providers failed.\"\n","meta":{"description":"FLUX.1 Schnell Manifold Function for Black Forest Lab Image Generation Model","manifest":{"title":"FLUX.1 Schnell Manifold Function for Black Forest Lab Image Generation Model","author":"bgeneto","author_url":"https://github.com/bgeneto/open-webui-flux-image-gen","funding_url":"https://github.com/open-webui","version":"0.2.1","license":"MIT","requirements":"pydantic, requests","environment_variables":"TOGETHER_API_KEY, HUGGINGFACE_API_KEY, REPLICATE_API_KEY","https":"//api.together.xyz/v1/images/generations"}},"is_active":true,"is_global":false,"updated_at":1729017599,"created_at":1728608603},{"id":"chat_context_clipper","user_id":"01503731-0a49-4033-bdc0-8c1bc3b75fd6","name":"Chat Context Clipper","type":"filter","content":"\"\"\"\ntitle: Chat Context Clipper (that works :-)\nauthor: open-webui & bgeneto (several improvements)\nauthor_url: https://github.com/bgeneto/open-webui-functions/blob/main/chat_context_clipper_function.py\nfunding_url: https://github.com/open-webui\nversion: 0.1.2\ndescription: A filter that truncates chat history to retain the latest n-th user and assistant\n             messages while always keeping the system prompt and also first message pair (if desired).\n             It ensures that the first message (after the prompt if any) is a user message (Anthropic requirement).\n             It also offers a user valve to set the number of messages to retain, which overrides the global setting.\n\"\"\"\n\nfrom typing import Dict, List, Optional, Union\n\nfrom pydantic import BaseModel, Field\n\nDEBUG = False\n\n\ndef get_first_user_message(\n    data: List[Dict[str, Union[str, dict]]]\n) -> Optional[Dict[str, str]]:\n    \"\"\"\n    Returns the first user message in the given data.\n\n    Args:\n        data (list): A list of dictionaries containing role and content.\n\n    Returns:\n        dict: The first user message.\n    \"\"\"\n    for message in data:\n        if message[\"role\"] == \"user\":\n            return message\n    return None\n\n\ndef get_first_assistant_message(\n    data: List[Dict[str, Union[str, dict]]]\n) -> Optional[Dict[str, str]]:\n    \"\"\"\n    Returns the first assistant message in the given data.\n\n    Args:\n        data (list): A list of dictionaries containing role and content.\n\n    Returns:\n        dict: The first assistant message.\n    \"\"\"\n    for message in data:\n        if message[\"role\"] == \"assistant\":\n            return message\n    return None\n\n\nclass Filter:\n    class Valves(BaseModel):\n        priority: int = Field(\n            default=0, description=\"Priority level for the filter operations\"\n        )\n        n_last_messages: int = Field(\n            default=4, description=\"Number of last messages to keep\"\n        )\n        keep_first: bool = Field(\n            default=True,\n            description=\"Always Keep the first user message and assistant answer\",\n        )\n        pass\n\n    class UserValves(BaseModel):\n        n_last_messages: int = Field(\n            default=4,\n            description=\"Number of last chat messages to keep in the assistant memory\",\n        )\n        pass\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.user_valves = self.UserValves()\n        pass\n\n    def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict:\n        messages = body[\"messages\"]\n        if DEBUG:\n            print(\"Original messages length:\", len(messages))\n\n        # Ensure we always keep the system prompt\n        system_prompt = next(\n            (message for message in messages if message.get(\"role\") == \"system\"), None\n        )\n\n        # Always keep the first user message...\n        first_user_message = get_first_user_message(messages)\n        # ...and the first assistant message\n        first_assistant_message = get_first_assistant_message(messages)\n\n        # Collect the last n_last_messages from user and assistant\n        n_last_messages = int(\n            self.user_valves.n_last_messages\n            if self.user_valves.n_last_messages\n            else self.valves.n_last_messages\n        )\n\n        # double (user and assistant) number of messages to keep and...\n        # ...also add one more pair of messages to account for keeping the first\n        n_last_messages = 2 * n_last_messages\n        if self.valves.keep_first:\n            n_last_messages = n_last_messages + 2\n\n        recent_messages = [\n            message for message in messages if message[\"role\"] in [\"user\", \"assistant\"]\n        ][-n_last_messages:]\n\n        # Construct the new message list by appending the system prompt first (if any)\n        new_messages = []\n        if system_prompt:\n            new_messages.append(system_prompt)\n\n        # Check if we need to append the first couple of messages\n        if self.valves.keep_first:\n            if first_user_message:\n                new_messages.append(first_user_message)\n            if first_assistant_message:\n                new_messages.append(first_assistant_message)\n\n        # Ensure the sequence is system -> user -> assistant\n        if (\n            recent_messages\n            and recent_messages[0][\"role\"] == \"user\"\n            and len(recent_messages) > 1\n        ):\n            if recent_messages[1][\"role\"] == \"user\":\n                recent_messages.pop(0)\n\n        # remove/pop assistant message if it the first\n        if (\n            recent_messages\n            and recent_messages[0][\"role\"] == \"assistant\"\n            and len(recent_messages) > 1\n        ):\n            recent_messages.pop(0)\n            if len(recent_messages) > 1 and recent_messages[0][\"role\"] == \"assistant\":\n                recent_messages.pop(0)\n\n        new_messages.extend(recent_messages)\n\n        if DEBUG:\n            print(\"Clipped messages length:\", len(new_messages))\n\n        # Update body messages\n        body[\"messages\"] = new_messages\n\n        return body\n","meta":{"description":"A filter that truncates chat history to retain the latest n-th user and assistant messages while always keeping the system prompt and also first message pair.","manifest":{"title":"Chat Context Clipper (that works :-)","author":"open-webui & bgeneto (several improvements)","author_url":"https://github.com/bgeneto/open-webui-functions/blob/main/chat_context_clipper_function.py","funding_url":"https://github.com/open-webui","version":"0.1.2","description":"A filter that truncates chat history to retain the latest n-th user and assistant"}},"is_active":false,"is_global":false,"updated_at":1728967612,"created_at":1728690298}]